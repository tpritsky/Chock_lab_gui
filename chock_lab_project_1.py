# -*- coding: utf-8 -*-
"""Chock_Lab_Project_1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s6F0BkbkT0DgV3SnKlj0Q24R8_LAhJMM
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd

#convert a file uploaded by user to a dataframe
df_1 = pd.read_csv("short_csv_with_header_1.csv")     #load file
df_2 = pd.read_csv("short_csv_with_header_2.csv")
df_3 = pd.read_csv("short_csv_with_header_3.csv")

print(df_2)

import pandas as pd
import numpy as np
import random
import statistics
from statistics import mean 
import csv

'''TO DO:
1. Remove decimal precision
2. Add date range
3. Expand entry to multiple data columns
'''

#User set variables: Include column names that should be adjusted to match columns in data
#________________________________________________________________________________________________________________
date_column_title = "Date" #used in organize_data_by_date_and_hour
time_column_title = "Time" #used in organize_data_by_date_and_hour
csat_column_title = "Csat" #used in organize_data_by_date_and_hour
rsat_column_title = "Rsat" #used in organize_data_by_date_and_hour
min_num_data_points = 10   #the minimum number of data points needed to compile an average (adjust by user);  used in compile_hourly_avg function
#_______________________________________________________________________________________________________________

#GUI set parameters: runtime params set by end user through GUI
#__________________________________________________________________________________________________________________
min_day = 0 #earliest date from which to report data
min_month = 0 #earliest date from which to report data
min_year = 2000 #earliest date from which to report data
min_hour = 0 #earliest date from which to report data

max_day = 10 #latest date from which to report data
max_month = 10 #earliest hour from which to report data
max_year = 2010 #latest date from which to report data
max_hour = 10 #latest hour from which to report data

num_hours_for_avg = 6 #number of hours to average data over
#__________________________________________________________________________________________________________________

#convert to dataframe function: converts excel uploads to dataframes
def convert_to_df(csv_file):
  df = pd.read_csv(csv_file, header=None)     #load file
  return df


#cocatenate function: Takes in a list of dataframes and returns a cocatenated dataframe
def concatenate_dataframes(list_of_dataframes):
  df_new = pd.concat(list_of_dataframes)
  return df_new

#sum list over range
def sum_list_over_range(input_list, start_index, end_index):
  sum = 0
  for i in range(int(start_index), int(end_index)):
    sum += input_list[i]
  return sum

#extract column: takes in a dataframe and column number (or column name) and returns a list of column entries
def extract_column(input_df, col_num):
  df_list = input_df[col_num].tolist()  
  return df_list

#average list: returns the average of a list, excluding zero entries
def list_avg(input_list):
  input_list = [i for i in input_list if i != 0]                #filter out zero entries
  return mean(input_list)

#hourly average: takes in a list of data entries, an integer indicator of time separation between each entry, in seconds (ie. 30 sec, 40 sec, etc), and returns a list consisting of hourly averages of the data. Trims off the final incomplete hour of data. 
def hourly_avg(input_list, time_sep):
  num_entries_per_hour = 3600/time_sep      #calculate the number of entries in an hour (an hour is 3600 seconds)
  num_hours = int(len(input_list)/float(num_entries_per_hour))      #number of hours in data set
  output_list = []     #stores list of hourly averages for the data

  list_index = 0;      #stores position in list
  for i in range(0, num_hours):
      hour_sum = sum_list_over_range(input_list,list_index,list_index + num_entries_per_hour)    #sum of entries over a given hour
      hour_avg = hour_sum/float(num_entries_per_hour)                         #average of entries over a given hour
      output_list.append(hour_avg)
      list_index = list_index + num_entries_per_hour                              #update position in list to next hour
  return output_list;

#export list to csv file as column:
def export_list_as_csv_column(input_list, output_file_name):
  with open(output_file_name,'w') as resultFile:
    wr = csv.writer(resultFile, dialect='excel')
    wr.writerow([input_list])
    
    
#return data as a dictionary of dictionaries; outer dictionary contains dates and inner dictionary contains hours; currently the data is just the sc values in column 3, but can be expanded to be a tuple of other data values
def organize_data_by_date_and_hour(input_df):
  #extract data from df
  dates_list = extract_column(input_df, date_column_title)
  times_list = extract_column(input_df, time_column_title)
  csat_list = extract_column(input_df, csat_column_title)    #extract csat data to list
  rsat_list = extract_column(input_df, rsat_column_title)    #extract rsat data to list
  
  daily_data = {}      #a dictionary of data entries; will contain an hourly data dictionary corresponding to each date
  
  #loop through the current hour
  my_counter = 0 
  for i in range(len(dates_list)):
    if type(dates_list[i]) == str and type(times_list[i]) == str:
      my_counter +=1
      current_date = dates_list[i]
      current_hour = times_list[i].split(':')[0]      #the current hour over which we are analyzing data
    
      if current_date not in daily_data:              #append the current date
        daily_data[current_date] = {} 
        daily_data[current_date][current_hour] = []
        daily_data[current_date][current_hour].append(csat_list[i])      #append the csat data
        
      else:
        if current_hour not in daily_data[current_date]:                #check if current hour already in dictionary
          daily_data[current_date][current_hour] = []
          daily_data[current_date][current_hour].append(csat_list[i])     #append the data
        else:
          daily_data[current_date][current_hour].append(csat_list[i])    #append the data
          #print(csat_list[i])
  print("Num valid non-zero elements: ")
  print(my_counter)
  return daily_data

#compile hourly averages and returns dataframe with headers (date, time, csat hourly average, num elems). Takes in dictionary (dates) of dictionaries (hours): 
def compile_hourly_avg(data_entry):
  hourly_averages = []    #store hourly averages
  
  for date_key, date_value in data_entry.items():
    for hour_key, hour_value in date_value.items():
      hour_sum = 0
      hour_num_nonzero_elems = 0
      for i in hour_value:
        if i != 0:
          hour_sum += int(i)
          hour_num_nonzero_elems += 1
      if hour_num_nonzero_elems >= min_num_data_points:
        hour_avg = hour_sum/hour_num_nonzero_elems
      else:
        hour_avg = 0
      hourly_averages.append((date_key, hour_key, hour_avg, hour_num_nonzero_elems))  
  dfObj = pd.DataFrame(hourly_averages, columns = [date_column_title, time_column_title, csat_column_title, 'Num Nonzero Elems'])
  return(dfObj)

#convert list of csv files to list of dataframes
def convert_csv_list_to_df_list(csv_list):
  df_list = []
  for i in csv_list:
    converted_df = pd.read_csv(i)
    df_list.append(converted_df)
  return df_list

#create list of csv files
#list_of_csv = ['181101N_DAL115 (1).csv', '181031N_DAL115.csv', '181030N_DAL115_with_header.csv']
list_of_csv = ['short_csv_with_header_3.csv', 'short_csv_with_header_2.csv', 'short_csv_with_header_1.csv']

#convert list of csv files to dataframes
list_of_df = convert_csv_list_to_df_list(list_of_csv)

#concatenate dataframes
concatenated_df = concatenate_dataframes(list_of_df)  

#reformat data as dictionary of dictionaries organized by dates and hours within dates
daily_data = organize_data_by_date_and_hour(concatenated_df)

#compile hourly averages and return dataframe containing hourly averages
hourly_averages = compile_hourly_avg(daily_data)
print(hourly_averages)

#export data to excel
export_csv = hourly_averages.to_csv("formatted_csat_output.csv", header=True)

#download from colab
from google.colab import files
files.download("formatted_csat_output.csv")

#extract data from df
dates_list = extract_column(df_1, 1)
times_list = extract_column(df_1, 2)
sc_list = extract_column(df_1, 3)

DAL115_csat = extract_column(df_2, 'Csat')
print(DAL115_csat)

'''_________________________________________________________________________________________________________'''

#data entry:
daily_data = {}      #a dictionary of data entries; will contain an hourly data dictionary corresponding to each date

'''_________________________________________________________________________________________________________'''

#loop through columns

#set hour
#current_hour = times_list[i].split(':')[0]      #the current hour over which we are analyzing data

counter = 0
#loop through the current hour
for i in range(len(dates_list)):
  current_date = dates_list[i]
  current_hour = times_list[i].split(':')[0]      #the current hour over which we are analyzing data
  
  if current_date not in daily_data:              #append the current date
    daily_data[current_date] = {} 
    daily_data[current_date][current_hour] = []
    daily_data[current_date][current_hour].append(sc_list[i])      #append the data
  else:
    if current_hour not in daily_data[current_date]:                #check if current hour already in dictionary
      daily_data[current_date][current_hour] = []
      daily_data[current_date][current_hour].append(sc_list[i])     #append the data
    else:
      daily_data[current_date][current_hour].append(sc_list[i])    #append the data
      #print(sc_list[i])
      counter += 1
print(counter)
print(daily_data.keys())

print(dates_list)
print(times_list)
print(sc_list)
print(daily_data)

#test function to see if correct number of items
counter_2 = 0
for i, j in daily_data['10/30/18'].items():
    print(j)
    for k in j:
      counter_2 +=1
    
print(counter_2)

#backup

#compile hourly averages and return dictionary (dates) of dictionaries (hours) containing hourly averages
hourly_averages = []    #store hourly averages
min_num_data_points = 20  #the minimum number of data points needed to compile an average (adjust by user)
print(daily_data)
print(concatenated_df)
#print(df_1)

for date_key, date_value in daily_data.items():
  print(date_value)
  for hour_key, hour_value in date_value.items():
    print(hour_value)
    hour_sum = 0
    hour_num_nonzero_elems = 0
    for i in hour_value:
      print(i)
      if i != 0:
        hour_sum += i
        hour_num_nonzero_elems += 1
    if hour_num_nonzero_elems >= min_num_data_points:
      hour_avg = hour_sum/hour_num_nonzero_elems
    else:
      hour_avg = 0
    hourly_averages.append((date_key, hour_key, hour_avg, hour_num_nonzero_elems))  
print(hourly_averages)
